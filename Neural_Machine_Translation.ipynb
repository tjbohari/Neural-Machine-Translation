{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYQt0nUEbAgN"
   },
   "source": [
    "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system with an attention model.\n",
    "\n",
    "This is an **individual assignment** and usual rules for plagiarism apply! With this you agree that: \"In submitting this work I confirm that it is entirely my own. I acknowledge that I may be invited to online interview if there is any concern in relation to the integrity of my exam.\" \n",
    "\n",
    "**Write comments and documentation.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d33hukl0w-Nh"
   },
   "source": [
    "## Section 1- Data Collection and Preprocessing \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAFJXKQaeN-d"
   },
   "source": [
    "**Task 1  (5 marks)**\n",
    "\n",
    "---\n",
    "\n",
    "There are few datasets to train an NMT system available from the OPUS project (http://opus.nlpl.eu/).\n",
    "\n",
    "*  Download a language pair (preferably European language) and **extract** the file(s) and upload it to CoLab\n",
    "*  Create a list of lines by splitting the text file at every occurrence accordingly, i.e. source and target language\n",
    "*  Print number of sentences\n",
    "*  Limit the number of sentences to 10,000 lines (but more than 5,000 lines)\n",
    "*  Split the data into train, development and test set\n",
    "*  Print 100th sentence in original script for source and target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4M4DSLtXG8U"
   },
   "outputs": [],
   "source": [
    "## importing required libraries\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Dropout, concatenate, dot, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U9BaH3ozUGf",
    "outputId": "7fd33dbb-eeae-4f45-a6e6-e1977d3f0ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7635\n",
      "100th line of source is &Keep report file for sending later or copying to somewhere else\n",
      "100th line of target is &Garder le fichier du rapport pour un envoi ultérieur ou pour le copier ailleurs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eng_filename = \"english.txt\"\n",
    "french_filename = \"french.txt\"\n",
    "\n",
    "# reading data from each file where lines are split using \"\\n\"\n",
    "with open(eng_filename , 'r') as english:\n",
    "    english_lines = english.read().split('\\n')\n",
    "\n",
    "with open(french_filename, 'r') as french:\n",
    "    french_lines = french.read().split('\\n')\n",
    "\n",
    "print(len(english_lines))\n",
    "\n",
    "\n",
    "english_lines, french_lines = np.array(english_lines), np.array(french_lines)\n",
    "\n",
    "# dividing the data in train, dev and test set\n",
    "random = np.random.randint(0, len(english_lines), len(english_lines))\n",
    "\n",
    "train_x, dev_x, test_x = english_lines[random[0:6000]],  english_lines[random[6000:6500]],  english_lines[random[6500:len(english_lines)]]\n",
    "train_y, dev_y, test_y = french_lines[random[0:6000]],  french_lines[random[6000:6500]],  french_lines[random[6500:len(english_lines)]]\n",
    "\n",
    "print(f\"100th line of source is {english_lines[100]}\")\n",
    "print(f\"100th line of target is {french_lines[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x63IEWjUxkJj"
   },
   "source": [
    "**Task 2 (5 marks)** \n",
    "\n",
    "---\n",
    "\n",
    "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
    "* Perform the pre-processing step of the text.\n",
    "* Print the last 5 sentences of the preprocessed text.\n",
    "* Print statistics on the selected dataset:\n",
    "  * Number of samples\n",
    "  * Number of unique source language tokens\n",
    "  * Number of unique target language tokens\n",
    "  * Max sequence length of source language\n",
    "  * Max sequence length of target language\n",
    "  * Source Vocabulary\n",
    "  * Target Vocabulary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmNr8gnzfCeS",
    "outputId": "a33a4645-e78a-40c7-a7d5-15817b0fa655",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 sentences are : \n",
      "\n",
      "['machines eos', 'Redémarrage dopenbsdinetd eos', 'Color name eos', 'Installer le micrologiciel eos', 'La clef avec lempreinte s n eos'] \n",
      "\n",
      "Statistics are : \n",
      "\n",
      "Number of samples 6000\n",
      "Numbe of unique source language tokens is 3579\n",
      "Numbe of unique target language tokens is 3943\n",
      "Max sequence length of source language is 8\n",
      "Max sequence length of target language is 10\n",
      "Source Vocabulary is 19970\n",
      "Target Vocabulary is 27643\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "source, target, target_input = list(), list(), list()\n",
    "source_words, target_words = list(), list()\n",
    "\n",
    "# taking first 30 chars from each line for faster processing\n",
    "for i in range(len(train_x)):\n",
    "    eng = train_x[i][:30]\n",
    "    frc =  train_y[i][:30] + ' <eos>'\n",
    "    output_sentence_input = '<bof> ' + train_y[i][:30]\n",
    "    eng = re.sub(r'[^\\w\\s]', '', eng)\n",
    "    frc = re.sub(r'[^\\w\\s]', '', frc)\n",
    "    source.append(eng)\n",
    "    target.append(frc)\n",
    "    for e in eng.split(\" \"):\n",
    "      source_words.append(e)\n",
    "    for t in frc.split(\" \"):\n",
    "      target_words.append(t)\n",
    "    target_input.append(output_sentence_input)\n",
    "    \n",
    "    \n",
    "max_source = max([len(txt.split(\" \")) for txt in source])\n",
    "max_target = max([len(txt.split(\" \")) for txt in target])\n",
    "\n",
    "vocab_source = len(source)\n",
    "vocab_target = len(target)\n",
    "\n",
    "print(\"Last 5 sentences are : \\n\")\n",
    "print(target[-5:], \"\\n\")\n",
    "\n",
    "print(\"Statistics are : \\n\")\n",
    "print(f\"Number of samples {len(source)}\")\n",
    "print(f\"Numbe of unique source language tokens is {len(set(source_words))}\")\n",
    "print(f\"Numbe of unique target language tokens is {len(set(target_words))}\")\n",
    "print(f\"Max sequence length of source language is {max_source}\")\n",
    "print(f\"Max sequence length of target language is {max_target}\")\n",
    "print(f\"Source Vocabulary is {len(source_words)}\")\n",
    "print(f\"Target Vocabulary is {len(target_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuQ7B78BhLrx"
   },
   "source": [
    "**Task 3 (5 marks)** \n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "*  Assign each unique word an integer value (5 marks).\n",
    "*  Create word embedding for your vocabulary using pre-trained embeddings, for example GloVe or fastText (10 marks) (https://nlp.stanford.edu/projects/glove/ , https://fasttext.cc/docs/en/english-vectors.html)\n",
    "* Print the first line of the embeddings (see below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L37XcTMBYpQN"
   },
   "outputs": [],
   "source": [
    "## initializing required values\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LSTM_NODES = 512\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPq-c1tqfoRO"
   },
   "outputs": [],
   "source": [
    "## assigning each unique word an integer\n",
    "input_token_index = dict([(word,i) for i, word in enumerate(source)])\n",
    "target_token_index= dict([(word,i) for i, word in enumerate(target)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-ybWu0tXG8a",
    "outputId": "d20cb485-05c1-4782-cd6b-0903834858ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 2755\n",
      "Length of longest sentence in input: 8\n",
      "Total unique words in the output: 3832\n",
      "Length of longest sentence in the output: 10\n"
     ]
    }
   ],
   "source": [
    "## tokenizing the text using Tokenizer from keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenizing input sentences\n",
    "MAX_NUM_WORDS = 20000\n",
    "source_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token=\"UNK\")\n",
    "source_tokenizer.fit_on_texts(source)\n",
    "input_integer_seq = source_tokenizer.texts_to_sequences(source)\n",
    "\n",
    "word2idx_inputs = source_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)\n",
    "\n",
    "# tokenizing output sentences\n",
    "target_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "target_tokenizer.fit_on_texts(target + target_input)\n",
    "output_integer_seq = target_tokenizer.texts_to_sequences(target)\n",
    "output_input_integer_seq = target_tokenizer.texts_to_sequences(target_input)\n",
    "\n",
    "word2idx_outputs = target_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDvuW7HkXG8b",
    "outputId": "f1ff6417-f9a7-4010-8d64-51bc7af8f5c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (6000, 10)\n",
      "encoder_input_sequences[172]: [  0   0   0   0   0  27 209 321  87   3]\n"
     ]
    }
   ],
   "source": [
    "# padding the sentences so all sentences are of equal length\n",
    "\n",
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=MAX_SENTENCE_LENGTH)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvRIW2NhXG8b",
    "outputId": "d8d18082-9883-483d-994d-bdd04b018024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (6000, 10)\n",
      "decoder_input_sequences[172]: [ 38  23   3   5 169   9   1   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# padding the sentences so all sentences are of equal length\n",
    "\n",
    "decoder_input_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVtVgzAbXG8c"
   },
   "outputs": [],
   "source": [
    "# creating embedding from Glove\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VuZC2a9XG8c"
   },
   "outputs": [],
   "source": [
    "# reading for each word it's embedding from dictionary\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(source) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in input_token_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXEQIAWmIyZc",
    "outputId": "b00c73c6-c2cd-4027-c473-7f93a1975341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing first line of embedding\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing first line of embedding\")\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCzehl04XG8d"
   },
   "outputs": [],
   "source": [
    "## creting one hot encoding for target decoder \n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(target),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "for i, d in enumerate(decoder_input_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QfLgKEgazro"
   },
   "source": [
    "## Section 2 Translation Model training\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8WnlX8d0RVj"
   },
   "source": [
    "**Task 4 (15 marks)**\n",
    "* Provide code for the encoder using Keras LSTM (5 marks)\n",
    "* Provide code for the decoder using Keras LSTM (5 marks)\n",
    "* Train the sequence2sequence (encoder-decoder) model (5 marks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uReDZ814XG8e"
   },
   "outputs": [],
   "source": [
    "## creating the embedding layer\n",
    "\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_sLH38o0BJq"
   },
   "outputs": [],
   "source": [
    "# encoder code goes here\n",
    "# creating the encoder\n",
    "encoder_inputs_placeholder = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "# adding dropout to prevent overfitting\n",
    "encoder = LSTM(LSTM_NODES, dropout=0.4, recurrent_dropout=0.4, return_state=True)\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "state = [h, c]\n",
    "encoder_states = state # keeping the encoder states for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erw3s9vI0scb"
   },
   "outputs": [],
   "source": [
    "# decoder code goes here\n",
    "# creating the decoder\n",
    "\n",
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "# adding dropout to prevent overfitting\n",
    "decoder_lstm = LSTM(LSTM_NODES, dropout=0.4, recurrent_dropout=0.4, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states) # keeping decoder outputs for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkJ-ZMdMXV5_"
   },
   "outputs": [],
   "source": [
    "# creating the dense layer for final layer of model\n",
    "\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')(decoder_outputs)\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xe3hV0b00uJN",
    "outputId": "75c0c3c3-6edf-4e52-8208-9b8443548bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 10, 100)      600100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 512)      1962496     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 1255424     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 10, 512), (N 2099200     embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10, 3833)     1966329     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,883,549\n",
      "Trainable params: 7,883,549\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating the model \n",
    "model= Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_dense,\n",
    ")\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovWQs8KG09we",
    "outputId": "748eac0f-e42d-4457-d8aa-74cd47b4e1df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 53s 1s/step - loss: 4.4802 - accuracy: 0.4991 - val_loss: 2.7726 - val_accuracy: 0.6460\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 48s 1s/step - loss: 2.4384 - accuracy: 0.6513 - val_loss: 2.2586 - val_accuracy: 0.6739\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 2.0745 - accuracy: 0.6832 - val_loss: 1.9777 - val_accuracy: 0.7233\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 1.7444 - accuracy: 0.7363 - val_loss: 1.6697 - val_accuracy: 0.7632\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 47s 1s/step - loss: 1.4135 - accuracy: 0.7767 - val_loss: 1.4869 - val_accuracy: 0.7974\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 1.1869 - accuracy: 0.8128 - val_loss: 1.3504 - val_accuracy: 0.8218\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.9971 - accuracy: 0.8395 - val_loss: 1.2083 - val_accuracy: 0.8440\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.8251 - accuracy: 0.8688 - val_loss: 1.1147 - val_accuracy: 0.8609\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 47s 1s/step - loss: 0.6972 - accuracy: 0.8933 - val_loss: 1.0364 - val_accuracy: 0.8757\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.5918 - accuracy: 0.9119 - val_loss: 0.9600 - val_accuracy: 0.8876\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.5000 - accuracy: 0.9253 - val_loss: 0.9078 - val_accuracy: 0.8997\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.4200 - accuracy: 0.9385 - val_loss: 0.8476 - val_accuracy: 0.9104\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.3459 - accuracy: 0.9514 - val_loss: 0.8030 - val_accuracy: 0.9184\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.2853 - accuracy: 0.9619 - val_loss: 0.7709 - val_accuracy: 0.9269\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.2292 - accuracy: 0.9722 - val_loss: 0.7276 - val_accuracy: 0.9321\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.1887 - accuracy: 0.9787 - val_loss: 0.7053 - val_accuracy: 0.9367\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.1495 - accuracy: 0.9848 - val_loss: 0.6727 - val_accuracy: 0.9403\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.1215 - accuracy: 0.9872 - val_loss: 0.6572 - val_accuracy: 0.9429\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.1020 - accuracy: 0.9905 - val_loss: 0.6387 - val_accuracy: 0.9453\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 46s 1s/step - loss: 0.0793 - accuracy: 0.9932 - val_loss: 0.6265 - val_accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# training the model using the values declared above\n",
    "model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.3,\n",
    ")\n",
    "model.save('seq2seq_source_target.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIvnxA6R2kBS"
   },
   "outputs": [],
   "source": [
    "# creating the encoder model with the inputs and the states\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMAOQrFp2jxD"
   },
   "outputs": [],
   "source": [
    "# creating the decoder states\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K1lJPhA6X-e"
   },
   "outputs": [],
   "source": [
    "# creating the decoder model\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "# decoder_states = d_state\n",
    "decoder_outputs = Dense(num_words_output, activation='softmax')(decoder_outputs)\n",
    "\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Sy_WCp31x79"
   },
   "source": [
    "## Section 3 Testing\n",
    "\n",
    "---\n",
    "\n",
    "**Task 5 (20 marks)**\n",
    "\n",
    "* Use the trained model to translate the text from the source into the target language (10 marks). \n",
    "* Use the test/evaluation set (see Section 1) and perform an automatic evaluation with the BLEU metric (10 marks). \n",
    "You can use the NLTK library to calculate BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4Zhkgxa8lFX"
   },
   "outputs": [],
   "source": [
    "# creating dictionary for index to words\n",
    "indextoword_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "indextoword_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AT2dmq61QtbW"
   },
   "outputs": [],
   "source": [
    "\"\"\" Below method takes the input sequence and tries to translate the sequence to target language\"\"\"\n",
    "\n",
    "def decode_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<bof>']\n",
    "    eos = word2idx_outputs['eos']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = indextoword_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0kslLpFC0to"
   },
   "outputs": [],
   "source": [
    "# taking test data and selecting first 30 chars\n",
    "test_x_processed = list()\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "  e = test_x[i][:30]\n",
    "  test_x_processed.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toVPmYWPCT9n",
    "outputId": "6c7d4bc6-6784-4579-e906-5b34d38f1865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 2982\n",
      "Length of longest sentence in input: 9\n"
     ]
    }
   ],
   "source": [
    "## preprocessing test data so it could be used to predict using the model\n",
    "source_tokenizer.fit_on_texts(test_x_processed)\n",
    "input_integer_seq = source_tokenizer.texts_to_sequences(test_x_processed)\n",
    "\n",
    "word2idx_inputs = source_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)\n",
    "\n",
    "test_encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=MAX_SENTENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e3BUwxbEfrc"
   },
   "outputs": [],
   "source": [
    "# translating sequences from test using the model\n",
    "normal_actual_sentence, normal_predicted_sentence = list(), list() \n",
    "for i in range(len(test_encoder_input_sequences)):\n",
    "    input_seq = test_encoder_input_sequences[i:i+1]\n",
    "    translation = decode_sentence(input_seq)\n",
    "    normal_actual_sentence.append(test_y[i])\n",
    "    normal_predicted_sentence.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGWg3jSvKx99",
    "outputId": "28d6683c-2fbb-4791-af87-80363bfc6f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Auto-frappes', 'Quitter', 'Le pair a été trouvé par échange de pairs (PEX)'] \n",
      " ['lobtention review débog six six lautofrappe fot v fot fot', 'aut détails vérifier radioactif radioactif retour répétition lexécut répétition lexécut', 'cou mio communauté communauté raid téléchargement soutien luminécran os luminécran']\n"
     ]
    }
   ],
   "source": [
    "print(normal_actual_sentence[:3],\"\\n\" ,normal_predicted_sentence[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpCAjkFtFOc9",
    "outputId": "5c57a301-e6df-40f2-ab5b-5b81b5a1d2b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38\n",
      "0.62\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "# calculating the bleu score\n",
    "import nltk\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(normal_actual_sentence, normal_predicted_sentence, weights=[0.5])\n",
    "print(round(BLEUscore, 2))\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(normal_actual_sentence, normal_predicted_sentence, weights=[0.25])\n",
    "print(round(BLEUscore, 2))\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(normal_actual_sentence, normal_predicted_sentence, weights=[1])\n",
    "print(round(BLEUscore, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb4F1-a00Hw6"
   },
   "source": [
    "# Section 4 Attention\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XTD-fCC1yUA"
   },
   "source": [
    "**Task 5 (40 Marks)** Sequence2Sequence\n",
    "\n",
    "* Extend the existing Seq2Seq model with an attention mechanism [Discussed in Class]\n",
    "* Create sequence2sequence model with attention (15 marks)\n",
    "* Train the model with the same data from Section 1 (10 marks)\n",
    "* Translate the evaluation set using the sequence2sequence attention model (10 marks)\n",
    "* Evaluate the translations made with the sequence2sequence attention model and compare it with the model without attention using BLEU (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cddDQ4923cPY"
   },
   "outputs": [],
   "source": [
    "# encoder code goes here\n",
    "# creating encoder for attention model\n",
    "encoder_inputs_placeholder = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, dropout=0.2, recurrent_dropout=0.2, return_state=True, return_sequences = True)\n",
    "encoder_stack_h, h, c = encoder(x)\n",
    "state = [h, c]\n",
    "encoder_states = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO8NwRT0n5LL"
   },
   "outputs": [],
   "source": [
    "# creating decoder model\n",
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_stack_h, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5waBiGvKBaj"
   },
   "outputs": [],
   "source": [
    "# creating attention mechanism\n",
    "\n",
    "attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = dot([attention, encoder_stack_h], axes=[2,1])\n",
    "decoder_outputs = concatenate([context, decoder_stack_h])\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgmRQ1euqtNx"
   },
   "outputs": [],
   "source": [
    "# creating the model\n",
    "\n",
    "model= Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_dense,\n",
    ")\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d8f3imFrQA8",
    "outputId": "2147ba6a-2bff-408f-f8c7-ec63ff369787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 63s 2s/step - loss: 4.4834 - accuracy: 0.4974 - val_loss: 2.6748 - val_accuracy: 0.5793\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 56s 2s/step - loss: 2.3932 - accuracy: 0.6450 - val_loss: 2.2455 - val_accuracy: 0.6849\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 2.0285 - accuracy: 0.6896 - val_loss: 1.8974 - val_accuracy: 0.7311\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 1.6430 - accuracy: 0.7424 - val_loss: 1.6046 - val_accuracy: 0.7734\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 1.3122 - accuracy: 0.7865 - val_loss: 1.4185 - val_accuracy: 0.8051\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 1.0477 - accuracy: 0.8245 - val_loss: 1.2669 - val_accuracy: 0.8282\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.8426 - accuracy: 0.8601 - val_loss: 1.1561 - val_accuracy: 0.8543\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.6752 - accuracy: 0.8909 - val_loss: 1.0614 - val_accuracy: 0.8692\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.5451 - accuracy: 0.9118 - val_loss: 0.9811 - val_accuracy: 0.8873\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.4354 - accuracy: 0.9314 - val_loss: 0.9083 - val_accuracy: 0.8996\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.3510 - accuracy: 0.9468 - val_loss: 0.8673 - val_accuracy: 0.9087\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.2843 - accuracy: 0.9589 - val_loss: 0.8057 - val_accuracy: 0.9191\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.2209 - accuracy: 0.9716 - val_loss: 0.7753 - val_accuracy: 0.9260\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 57s 2s/step - loss: 0.1696 - accuracy: 0.9800 - val_loss: 0.7246 - val_accuracy: 0.9296\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.1395 - accuracy: 0.9846 - val_loss: 0.7157 - val_accuracy: 0.9330\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.1085 - accuracy: 0.9894 - val_loss: 0.6955 - val_accuracy: 0.9373\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.0870 - accuracy: 0.9923 - val_loss: 0.6587 - val_accuracy: 0.9394\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.0637 - accuracy: 0.9942 - val_loss: 0.6507 - val_accuracy: 0.9411\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.0496 - accuracy: 0.9964 - val_loss: 0.6237 - val_accuracy: 0.9426\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 58s 2s/step - loss: 0.0391 - accuracy: 0.9974 - val_loss: 0.6260 - val_accuracy: 0.9419\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.3,\n",
    ")\n",
    "model.save('seq2seq_source_target.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhQJcKLhrQA9"
   },
   "outputs": [],
   "source": [
    "#creating the encoder model using the inputs and states\n",
    "attention_encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mm1iRbh7rQA-"
   },
   "outputs": [],
   "source": [
    "# creating decoder inputs \n",
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_states_inputs = [decoder_state_input_h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMwhXuFTrQA-"
   },
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOkWSF6CrQA-"
   },
   "outputs": [],
   "source": [
    "# creating decoder model for attention\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "# decoder_states = d_state\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')(decoder_outputs)\n",
    "attention_decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_dense] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68B74Q0yFBG7"
   },
   "outputs": [],
   "source": [
    "\"\"\" Below method takes an input sequence and translates it to target language\"\"\"\n",
    "\n",
    "def decode_sentence(input_seq):\n",
    "    states_value = attention_encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<bof>']\n",
    "    eos = word2idx_outputs['eos']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = attention_decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = indextoword_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wu67JskFJPB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# translating sequences from test using the model\n",
    "actual_sentence, predicted_sentence = list(), list() \n",
    "for i in range(len(test_encoder_input_sequences)):\n",
    "    input_seq = test_encoder_input_sequences[i:i+1]\n",
    "    translation = decode_sentence(input_seq)\n",
    "    actual_sentence.append(test_y[i])\n",
    "    predicted_sentence.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fTICLYVFJPD",
    "outputId": "441d5f54-f1f6-4621-956f-95fbc35b5f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38\n",
      "0.61\n",
      "0.14\n"
     ]
    }
   ],
   "source": [
    "# calculating bleu score using the attention model\n",
    "\n",
    "import nltk\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(actual_sentence, predicted_sentence, weights=[0.5])\n",
    "print(round(BLEUscore, 2))\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(actual_sentence, predicted_sentence, weights=[0.25])\n",
    "print(round(BLEUscore, 2))\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu(actual_sentence, predicted_sentence, weights=[1])\n",
    "print(round(BLEUscore, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86FdR9v4DM59"
   },
   "source": [
    "The bleu score has not changed much after implementing attention model. The attention model in our case is performing worse in terms of bleu score. Possible reasons for this could be less data for training or less epochs for training.\n",
    "\n",
    "The model is overfitted in both the cases. This could be one of the reason for poor results.\n",
    "\n",
    "The bleu score of model without attention with weight = 1 is 0.15.\n",
    "\n",
    "The bleu score for the attention model with weight = 1 is 0.14."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2_Neural_Machine_Translation_(NMT)(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
